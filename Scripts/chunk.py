import os
import sys
import json
from dotenv import load_dotenv
from tree_sitter import Language, Parser
import tree_sitter_python as tspython

from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# ========================== CONFIG ==========================
print("üîß Loading environment variables...")
load_dotenv()

print("üß† Initializing embedding model...")
embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"}
)

print("üå≤ Setting up Tree-sitter Python parser...")
PY_LANGUAGE = Language(tspython.language())
parser = Parser(PY_LANGUAGE)

# ========================== HELPERS ==========================

def load_code_from_file(filepath):
    print(f"üìÇ Loading code from file: {filepath}")
    with open(filepath, 'r', encoding='utf-8') as f:
        code = f.read()
    print(f"‚úÖ Code loaded from {filepath} ({len(code)} characters)")
    return code

def get_code_segment(code_bytes, node):
    return code_bytes[node.start_byte:node.end_byte].decode()

def extract_docstring_from_body(body_node, code_bytes): #finds comments
    if len(body_node.children) > 0 and body_node.children[0].type == "expression_statement":
        expr_node = body_node.children[0]
        if expr_node.children and expr_node.children[0].type == "string":
            return get_code_segment(code_bytes, expr_node.children[0])
    return None

# ======================= CODE PARSING ========================

def extract_elements(code_str):
    print("üîç Parsing code and extracting elements...")
    code_bytes = code_str.encode()
    tree = parser.parse(code_bytes)
    root_node = tree.root_node
    elements = []

    def walk(node, parent=None):
        if node.type == "function_definition":
            name_node = node.child_by_field_name("name")
            params_node = node.child_by_field_name("parameters")
            body_node = node.child_by_field_name("body")

            name = get_code_segment(code_bytes, name_node)
            full_code = get_code_segment(code_bytes, node)
            docstring = extract_docstring_from_body(body_node, code_bytes)

            parent_context = name
            print(f"üîß Found function: {name}")
            elements.append({
                "type": "function",
                "name": name,
                "code_chunk": full_code,
                "parent": parent,
                "summary": docstring or "Function with no docstring",
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1
            })
            for child in node.children:
                walk(child, parent_context)
            return

        elif node.type == "class_definition":
            name_node = node.child_by_field_name("name")
            body_node = node.child_by_field_name("body")

            name = get_code_segment(code_bytes, name_node)
            full_code = get_code_segment(code_bytes, node)
            docstring = extract_docstring_from_body(body_node, code_bytes)

            summary = f"Class `{name}`"
            if docstring:
                summary += f": {docstring.strip('\"')}"
            else:
                summary += " with methods or properties."

            parent_context = name
            print(f"üèõÔ∏è Found class: {name}")
            elements.append({
                "type": "class",
                "name": name,
                "code_chunk": full_code,
                "summary": summary,
                "parent": parent,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
            })
            for child in node.children:
                walk(child, parent_context)
            return

        elif node.type == "expression_statement":
            child = node.children[0]
            if child.type == "comment":
                comment_text = get_code_segment(code_bytes, child)
                print(f"üí¨ Found comment")
                elements.append({
                    "type": "comment",
                    "code_chunk": comment_text,
                    "summary": f"Comment: {comment_text.strip('# ').strip()}",
                    "parent": parent,
                    "start_line": node.start_point[0] + 1,
                    "end_line": node.end_point[0] + 1,
                })
            elif child.type == "call":
                call_text = get_code_segment(code_bytes, child)
                print(f"üìû Found function call")
                elements.append({
                    "type": "function_call",
                    "code_chunk": call_text,
                    "summary": f"Function call: {call_text}",
                    "parent": parent,
                    "start_line": node.start_point[0] + 1,
                    "end_line": node.end_point[0] + 1,
                })

        elif node.type == "assignment":
            code_snippet = get_code_segment(code_bytes, node)
            print(f"üìù Found assignment")
            elements.append({
                "type": "assignment",
                "code_chunk": code_snippet,
                "summary": f"Variable assignment: {code_snippet}",
                "parent": parent,
                "start_line": node.start_point[0] + 1,
                "end_line": node.end_point[0] + 1,
            })

        for child in node.children:
            walk(child, parent)

    walk(root_node)
    print(f"‚úÖ Extracted {len(elements)} code elements")
    return elements

# ======================= FAISS INDEXING ========================

def build_faiss_from_chunks(chunks):
    print("üß† Creating FAISS index from extracted chunks...")
    docs = [Document(page_content=chunk["code_chunk"], metadata=chunk) for chunk in chunks]
    vectordb = FAISS.from_documents(docs, embedding)
    vectordb.save_local("code_index_faiss")

    print("üíæ Saving metadata to index_metadata.json...")
    with open("index_metadata.json", "w") as f:
        json.dump(chunks, f, indent=2)

    print(f"‚úÖ Saved FAISS index and metadata for {len(docs)} chunks.")

# ============================ MAIN ============================

def process_file(filepath):
    code = load_code_from_file(filepath)
    elements = extract_elements(code)
    for elem in elements:
        elem["filename"] = filepath  # üóÇÔ∏è Add filename to chunk metadata
    return elements

def process_directory(dirpath):
    print(f"üìÅ Scanning directory: {dirpath}")
    all_chunks = []
    for root, _, files in os.walk(dirpath):
        for file in files:
            if file.endswith(".py"):
                full_path = os.path.join(root, file)
                all_chunks.extend(process_file(full_path))
    return all_chunks

if __name__ == "__main__":
    print("üöÄ Starting code indexing...")
    if len(sys.argv) < 2:
        print("‚ùå Usage: python code_indexer.py <file_or_folder_path>")
        sys.exit(1)

    input_path = sys.argv[1]
    all_elements = []

    if os.path.isdir(input_path):
        all_elements = process_directory(input_path)
    elif os.path.isfile(input_path) and input_path.endswith(".py"):
        all_elements = process_file(input_path)
    else:
        print("‚ùå Invalid input. Please provide a .py file or a folder.")
        sys.exit(1)

    build_faiss_from_chunks(all_elements)
    print("üèÅ Done!")
